{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import pickle\n",
    "\n",
    "# ðŸ”¹ ì£¼ì‹ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°\n",
    "tickers = [\n",
    "    \"AAPL\",  # ì• í”Œ\n",
    "    \"MSFT\",  # ë§ˆì´í¬ë¡œì†Œí”„íŠ¸\n",
    "    \"GOOGL\",  # êµ¬ê¸€(ì•ŒíŒŒë²³)\n",
    "]\n",
    "\n",
    "\n",
    "# # ðŸ”¹ ì£¼ì‹ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°\n",
    "# tickers = [\n",
    "#     \"AAPL\",  # ì• í”Œ\n",
    "#     \"MSFT\",  # ë§ˆì´í¬ë¡œì†Œí”„íŠ¸\n",
    "#     \"GOOGL\",  # êµ¬ê¸€(ì•ŒíŒŒë²³)\n",
    "#     \"TSLA\",  # í…ŒìŠ¬ë¼\n",
    "#     \"AMZN\",  # ì•„ë§ˆì¡´\n",
    "#     \"META\",  # ë©”íƒ€ (êµ¬ íŽ˜ì´ìŠ¤ë¶)\n",
    "#     \"NVDA\",  # ì—”ë¹„ë””ì•„\n",
    "#     \"NFLX\",  # ë„·í”Œë¦­ìŠ¤\n",
    "#     \"AMD\",  # AMD\n",
    "#     \"INTC\",  # ì¸í…”\n",
    "#     \"PYPL\",  # íŽ˜ì´íŒ”\n",
    "#     \"DIS\",  # ë””ì¦ˆë‹ˆ (ì—”í„°í…Œì¸ë¨¼íŠ¸)\n",
    "#     \"PEP\",  # íŽ©ì‹œì½” (ì†Œë¹„ìž¬)\n",
    "#     \"KO\",  # ì½”ì¹´ì½œë¼ (ì†Œë¹„ìž¬)\n",
    "#     \"XOM\",  # ì—‘ìŠ¨ëª¨ë¹Œ (ì—ë„ˆì§€)\n",
    "#     \"PFE\"   # í™”ì´ìž (ì œì•½)\n",
    "# ]\n",
    "\n",
    "\n",
    "df_list = []\n",
    "\n",
    "for ticker in tickers:\n",
    "    stock = yf.Ticker(ticker)\n",
    "    temp = stock.history(period=\"max\")[[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]]\n",
    "    temp[\"Ticker\"] = ticker\n",
    "    df_list.append(temp)\n",
    "\n",
    "# ðŸ”¹ ë°ì´í„° ë³‘í•©\n",
    "df = pd.concat(df_list)\n",
    "\n",
    "# ðŸ”¹ ë¡œê·¸ ë³€í™˜ ì ìš© (âœ… MinMax ì •ê·œí™” ì œê±°)\n",
    "df[\"Close\"] = np.log1p(df[\"Close\"])  # ë¡œê·¸ ë³€í™˜ë§Œ ì ìš©\n",
    "\n",
    "# ðŸ”¹ ì´ë™ í‰ê·  ë° ì§€í‘œ ì¶”ê°€\n",
    "df[\"MA_10\"] = df.groupby(\"Ticker\")[\"Close\"].transform(lambda x: x.rolling(window=10).mean())\n",
    "df[\"MA_20\"] = df.groupby(\"Ticker\")[\"Close\"].transform(lambda x: x.rolling(window=20).mean())\n",
    "df[\"STD_10\"] = df.groupby(\"Ticker\")[\"Close\"].transform(lambda x: x.rolling(window=10).std())\n",
    "df[\"STD_20\"] = df.groupby(\"Ticker\")[\"Close\"].transform(lambda x: x.rolling(window=20).std())\n",
    "\n",
    "# ðŸ”¹ RSI ì§€í‘œ ì¶”ê°€\n",
    "def compute_rsi(data, window=14):\n",
    "    delta = data.diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
    "    rs = gain / loss\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "df[\"RSI_14\"] = df.groupby(\"Ticker\")[\"Close\"].transform(lambda x: compute_rsi(x))\n",
    "df[\"MACD\"] = df.groupby(\"Ticker\")[\"Close\"].transform(lambda x: x.ewm(span=12, adjust=False).mean() - x.ewm(span=26, adjust=False).mean())\n",
    "df[\"MACD_Signal\"] = df.groupby(\"Ticker\")[\"MACD\"].transform(lambda x: x.ewm(span=9, adjust=False).mean())\n",
    "\n",
    "# ðŸ”¹ ê±°ëž˜ëŸ‰ ì´ë™ í‰ê·  ë° ë¡œê·¸ ë³€í™˜\n",
    "df[\"Volume_MA_10\"] = df.groupby(\"Ticker\")[\"Volume\"].transform(lambda x: x.rolling(window=10).mean())\n",
    "df[\"Volume_MA_20\"] = df.groupby(\"Ticker\")[\"Volume\"].transform(lambda x: x.rolling(window=20).mean())\n",
    "df[\"Log_Volume\"] = np.log1p(df[\"Volume\"])\n",
    "\n",
    "# ðŸ”¹ NaN ê°’ ì œê±°\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# âœ… í•„ìš”í•˜ì§€ ì•Šì€ ì»¬ëŸ¼ ì‚­ì œ (Ticker ì‚­ì œ)\n",
    "df.drop(columns=[\"Open\", \"High\", \"Low\", \"Volume\", \"Ticker\"], errors='ignore', inplace=True)\n",
    "\n",
    "df_standardized = df.copy()\n",
    "df_standardized[df.columns.difference([\"Close\"])] =  np.log1p(df.drop(columns=[\"Close\"]))  # âœ… Close ì œì™¸ í›„ ì •ê·œí™”\n",
    "\n",
    "df_standardized[\"Close\"] = df[\"Close\"]  # âœ… CloseëŠ” ì •ê·œí™”í•˜ì§€ ì•Šê³  ë¡œê·¸ ë³€í™˜ëœ ê°’ ê·¸ëŒ€ë¡œ ì‚¬ìš©\n",
    "df_standardized1 = df_standardized.copy()\n",
    "\n",
    "\n",
    "# ðŸ”¹ 50ì¼ ë‹¨ìœ„ ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„±\n",
    "sequence_length = 100\n",
    "output_length = 100\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in range(len(df_standardized) - sequence_length - output_length):\n",
    "    X.append(df_standardized.iloc[i : i + sequence_length].values)  \n",
    "    y.append(df_standardized.iloc[i + sequence_length : i + sequence_length + output_length][\"Close\"].values)  \n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# ðŸ”¹ ë°ì´í„° ë¶„í• \n",
    "split_idx = int(len(X) * 0.8)\n",
    "X_train, X_val = X[:split_idx], X[split_idx:]\n",
    "y_train, y_val = y[:split_idx], y[split_idx:]\n",
    "\n",
    "# ðŸ”¹ PyTorch í…ì„œ ë³€í™˜\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn_model(model_class, patience=100) : \n",
    "    # âœ… GPU ë©”ëª¨ë¦¬ ì •ë¦¬ (ë¶ˆí•„ìš”í•œ ìºì‹œ í•´ì œ)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # ðŸ”¹ ëª¨ë¸ ì´ˆê¸°í™”\n",
    "    model = model_class.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # âœ… í•™ìŠµ ë£¨í”„ ì¡°ê¸° ì¢…ë£Œ ê¸°ì¤€\n",
    "    best_val_rmse = float(\"inf\")\n",
    "    trigger_count = 0\n",
    "\n",
    "    # âœ… í‰ê·  ì£¼ê°€ ê³„ì‚° (ë¡œê·¸ ë³€í™˜ ì „ ë³µì›)\n",
    "    avg_stock_price_exp = np.expm1(df[\"Close\"]).mean()  # ë¡œê·¸ ë³€í™˜ ë³µì›í•œ í‰ê·  ì£¼ê°€\n",
    "\n",
    "    # ðŸ”¹ í•™ìŠµ ë£¨í”„\n",
    "    for epoch in range(500):\n",
    "        optimizer.zero_grad()\n",
    "        model.train()\n",
    "        predictions = model(X_train_tensor)\n",
    "        loss = criterion(predictions, y_train_tensor)\n",
    "        rmse = torch.sqrt(loss.mean())\n",
    "        mape = torch.mean(torch.abs((predictions - y_train_tensor) / y_train_tensor)) * 100\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_predictions = model(X_val_tensor)\n",
    "            val_loss = criterion(val_predictions, y_val_tensor)\n",
    "            val_rmse = torch.sqrt(val_loss.mean())\n",
    "            val_mape = torch.mean(torch.abs((val_predictions - y_val_tensor) / y_val_tensor)) * 100\n",
    "\n",
    "        # âœ… RMSE ë¡œê·¸ ë³€í™˜ ë³µì› í›„ í¼ì„¼íŠ¸ ê³„ì‚°\n",
    "        rmse_exp = np.expm1(val_rmse.item())  # RMSE ê°’ì„ ë¡œê·¸ ë³€í™˜ ì´ì „ ê°’ìœ¼ë¡œ ë³µì›\n",
    "        rmse_percentage = (rmse_exp / avg_stock_price_exp) * 100  # í¼ì„¼íŠ¸ ê³„ì‚°\n",
    "\n",
    "        if val_rmse.item() < best_val_rmse:\n",
    "            best_val_rmse = val_rmse.item()\n",
    "            trigger_count = 0\n",
    "            torch.save(model.state_dict(), \"best_stock_predictor.pth\")  \n",
    "        else:\n",
    "            trigger_count += 1\n",
    "        \n",
    "        if trigger_count >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1} due to no improvement in Val RMSE\")\n",
    "            break\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/500], RMSE: {rmse.item():.4f}, MAPE: {mape.item():.2f}%, \"\n",
    "                f\"Val RMSE: {val_rmse.item():.4f} (Converted: {rmse_exp:.2f}), \"\n",
    "                f\"Val RMSE %: {rmse_percentage:.2f}%, Val MAPE: {val_mape.item():.2f}%\")\n",
    "            \n",
    "    del model\n",
    "    del optimizer\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ìˆœì • Transformer ëª¨ë¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# âœ… í•™ìŠµ ë£¨í”„ ì¡°ê¸° ì¢…ë£Œ ê¸°ì¤€\n",
    "best_val_rmse = float(\"inf\")\n",
    "patience = 100\n",
    "trigger_count = 0\n",
    "\n",
    "# ðŸ”¹ Transformer ëª¨ë¸ ì •ì˜\n",
    "class GPTStockPredictor(nn.Module):\n",
    "    def __init__(self, input_dim=11, embed_dim=32, num_heads=8, ff_dim=128, num_layers=1, output_days=100):\n",
    "        super(GPTStockPredictor, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, embed_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=ff_dim)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(embed_dim, output_days)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.mean(dim=0)\n",
    "        output = self.fc(x)\n",
    "        return output\n",
    "\n",
    "# ðŸ”¹ ëª¨ë¸ ì´ˆê¸°í™”\n",
    "model_class = GPTStockPredictor(output_days=100).to(device)\n",
    "learn_model(model_class, patience=100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN ë ˆì´ì–´ ì¶”ê°€ transformer ëª¨ë¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_GPTStockPredictor(nn.Module):\n",
    "    def __init__(self, input_dim=11, embed_dim=16, num_heads=8, ff_dim=64, num_layers=1, output_days=50):\n",
    "        super(CNN_GPTStockPredictor, self).__init__()\n",
    "\n",
    "        # âœ… 1D CNN Feature Extractor\n",
    "        self.conv1 = nn.Conv1d(in_channels=input_dim, out_channels=embed_dim, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(in_channels=input_dim, out_channels=embed_dim, kernel_size=5, padding=2)\n",
    "        self.conv3 = nn.Conv1d(in_channels=input_dim, out_channels=embed_dim, kernel_size=7, padding=3)\n",
    "        self.conv_merge = nn.Linear(embed_dim * 3, embed_dim)\n",
    "        # self.conv_merge = nn.Linear(embed_dim , embed_dim)\n",
    "\n",
    "        # âœ… Transformer Encoder\n",
    "        self.embedding = nn.Linear(embed_dim, embed_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=ff_dim)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # âœ… Fully Connected Layer\n",
    "        self.fc = nn.Linear(embed_dim, output_days)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 1D CNN Feature Extraction\n",
    "        x = x.permute(0, 2, 1)  # (batch_size, input_dim, sequence_length)\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.conv2(x)\n",
    "        x3 = self.conv3(x)\n",
    "        x = torch.cat([x1, x2, x3], dim=1)  # (batch_size, embed_dim * 3, sequence_length)\n",
    "        # x = torch.cat([x1], dim=1)  # (batch_size, embed_dim * 3, sequence_length)\n",
    "        x = x.permute(0, 2, 1)  # (batch_size, sequence_length, embed_dim * 3)\n",
    "        x = self.conv_merge(x)  # (batch_size, sequence_length, embed_dim)\n",
    "\n",
    "        # Transformer Encoder\n",
    "        x = self.embedding(x)\n",
    "        x = x.permute(1, 0, 2)  # (sequence_length, batch_size, embed_dim)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.mean(dim=0)\n",
    "\n",
    "        # Output Layer\n",
    "        output = self.fc(x)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rkdalsghrj/anaconda3/envs/finpredictor/lib/python3.10/site-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/500], RMSE: 2.7394, MAPE: 154.76%, Val RMSE: 3.4668 (Converted: 31.03), Val RMSE %: 74.11%, Val MAPE: 93.44%\n",
      "Epoch [20/500], RMSE: 2.6170, MAPE: 149.60%, Val RMSE: 3.3076 (Converted: 26.32), Val RMSE %: 62.85%, Val MAPE: 88.27%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ðŸ”¹ ëª¨ë¸ ì´ˆê¸°í™”\n",
    "model_class = CNN_GPTStockPredictor(output_days=100).to(device)\n",
    "learn_model(model_class, patience=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RelativePOsitional ì‘ìš© ëª¨ë¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "class RelativePositionalEncoding(nn.Module):\n",
    "    def __init__(self, embed_dim, max_len=5000):\n",
    "        super(RelativePositionalEncoding, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        pe = torch.zeros(max_len, embed_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_dim, 2).float() * (-np.log(10000.0) / embed_dim))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = pe.unsqueeze(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:x.size(0), :]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ì„±ëŠ¥í™•ì¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error\n",
    "\n",
    "# ðŸ”¹ ë””ë°”ì´ìŠ¤ ì„¤ì •\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ðŸ”¹ Transformer ëª¨ë¸ í´ëž˜ìŠ¤ ì •ì˜ (í•™ìŠµí•œ ëª¨ë¸ê³¼ ë™ì¼í•œ êµ¬ì¡°)\n",
    "class GPTStockPredictor(nn.Module):\n",
    "    def __init__(self, input_dim=11, embed_dim=32, num_heads=8, ff_dim=128, num_layers=1, output_days=50):\n",
    "        super(GPTStockPredictor, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, embed_dim)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=ff_dim)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc = nn.Linear(embed_dim, output_days)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x.permute(1, 0, 2)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.mean(dim=0)\n",
    "        output = self.fc(x)\n",
    "        return output\n",
    "\n",
    "# ðŸ”¹ ì €ìž¥ëœ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "model = GPTStockPredictor(output_days=100).to(device)\n",
    "model.load_state_dict(torch.load(\"best_stock_predictor.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# ðŸ”¹ AAPL ì£¼ì‹ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "new_ticker = \"Googl\"\n",
    "stock = yf.Ticker(new_ticker)\n",
    "temp = stock.history(period=\"max\")[[\"Open\", \"High\", \"Low\", \"Close\", \"Volume\"]]\n",
    "\n",
    "# ðŸ”¹ ë¡œê·¸ ë³€í™˜ ì ìš© (âœ… MinMax ì •ê·œí™” ì œê±°)\n",
    "df = temp.copy()\n",
    "df[\"Close\"] = np.log1p(df[\"Close\"])  # Close ê°’ ë¡œê·¸ ë³€í™˜\n",
    "\n",
    "# ðŸ”¹ ì´ë™ í‰ê·  ë° ì§€í‘œ ì¶”ê°€\n",
    "df[\"MA_10\"] = df[\"Close\"].rolling(window=10).mean()\n",
    "df[\"MA_20\"] = df[\"Close\"].rolling(window=20).mean()\n",
    "df[\"STD_10\"] = df[\"Close\"].rolling(window=10).std()\n",
    "df[\"STD_20\"] = df[\"Close\"].rolling(window=20).std()\n",
    "\n",
    "# ðŸ”¹ RSI ê³„ì‚°\n",
    "def compute_rsi(data, window=14):\n",
    "    delta = data.diff()\n",
    "    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()\n",
    "    rs = gain / loss\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "df[\"RSI_14\"] = compute_rsi(df[\"Close\"], window=14)\n",
    "df[\"MACD\"] = df[\"Close\"].ewm(span=12, adjust=False).mean() - df[\"Close\"].ewm(span=26, adjust=False).mean()\n",
    "df[\"MACD_Signal\"] = df[\"MACD\"].ewm(span=9, adjust=False).mean()\n",
    "\n",
    "# ðŸ”¹ ê±°ëž˜ëŸ‰ ì´ë™ í‰ê·  ë° ë¡œê·¸ ë³€í™˜\n",
    "df[\"Volume_MA_10\"] = df[\"Volume\"].rolling(window=10).mean()\n",
    "df[\"Volume_MA_20\"] = df[\"Volume\"].rolling(window=20).mean()\n",
    "df[\"Log_Volume\"] = np.log1p(df[\"Volume\"])\n",
    "\n",
    "# ðŸ”¹ NaN ê°’ ì œê±°\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "# ðŸ”¹ ì €ìž¥ëœ ì •ê·œí™” ê°ì²´ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "with open(\"scaler.pkl\", \"rb\") as f:\n",
    "    scaler = pickle.load(f)\n",
    "# âœ… í•™ìŠµ ì‹œ ì‚¬ìš©í•œ ì»¬ëŸ¼ í™•ì¸\n",
    "train_columns = ['Close','MA_10', 'MA_20', 'STD_10', 'STD_20', 'RSI_14', 'MACD', 'MACD_Signal', 'Volume_MA_10', 'Volume_MA_20', 'Log_Volume']\n",
    "print(\"í›ˆë ¨ ë°ì´í„°ì— ì‚¬ìš©ëœ ì»¬ëŸ¼:\", train_columns)\n",
    "\n",
    "# âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ í•™ìŠµ ë•Œ ì‚¬ìš©í•œ ì»¬ëŸ¼ë§Œ ìœ ì§€\n",
    "df_close = df[[\"Close\"]].copy()  # Close ê°’ ë”°ë¡œ ì €ìž¥ (ë¡œê·¸ ë³€í™˜ëœ ìƒíƒœ)\n",
    "df = df[train_columns]  # í›ˆë ¨ì— ì‚¬ìš©ëœ ì»¬ëŸ¼ë§Œ ì„ íƒ\n",
    "print(\"í…ŒìŠ¤íŠ¸ ë°ì´í„° ì»¬ëŸ¼ í™•ì¸:\", df.columns.tolist())  # í™•ì¸ìš© ì¶œë ¥\n",
    "\n",
    "\n",
    "\n",
    "# ðŸ”¹ ì €ìž¥ëœ ì •ê·œí™” ê°ì²´ ì ìš© (âœ… Close ê°’ì€ ì •ê·œí™” ì•ˆ í•¨)\n",
    "df_standardized = pd.DataFrame(np.log1p(df), index=df.index, columns=df.columns)\n",
    "\n",
    "df_standardized[df.columns.difference([\"Close\"])] =  np.log1p(df.drop(columns=[\"Close\"]))  # âœ… Close ì œì™¸ í›„ ì •ê·œí™”\n",
    "df_standardized[\"Close\"] = df_close[\"Close\"]  # CloseëŠ” ì •ê·œí™”í•˜ì§€ ì•Šê³  ë¡œê·¸ ë³€í™˜ëœ ê°’ ìœ ì§€\n",
    "\n",
    "# ðŸ”¹ ìµœê·¼ 50ì¼ ë°ì´í„°ë¥¼ ìž…ë ¥ ë°ì´í„°ë¡œ ì‚¬ìš©\n",
    "sequence_length = 100\n",
    "X_test = df_standardized.iloc[-sequence_length*2-1:-(sequence_length + 1)].values  # ìµœê·¼ 50ì¼ ë°ì´í„°\n",
    "X_test = np.expand_dims(X_test, axis=0)  # ë°°ì¹˜ ì°¨ì› ì¶”ê°€\n",
    "X_test_tensor_2 = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "\n",
    "# ðŸ”¹ ì˜ˆì¸¡ ìˆ˜í–‰\n",
    "with torch.no_grad():\n",
    "    predicted = model(X_test_tensor_2).cpu().numpy().flatten()\n",
    "\n",
    "# âœ… ì˜ˆì¸¡ëœ ê°’ ë³µì› (ë¡œê·¸ ë³€í™˜ ì—­ë³€í™˜)\n",
    "predicted_prices = np.expm1(predicted)  # âœ… np.expm1() ì‚¬ìš©í•˜ì—¬ ì›ëž˜ ê°€ê²©ìœ¼ë¡œ ë³€í™˜\n",
    "\n",
    "# âœ… ì‹¤ì œ ë°ì´í„°ëŠ” ì›ë³¸ ê·¸ëŒ€ë¡œ ì‚¬ìš©\n",
    "actual_prices = temp[\"Close\"].iloc[-(sequence_length+1):-1].values\n",
    "\n",
    "# âœ… RMSE ê³„ì‚°\n",
    "rmse = np.sqrt(mean_squared_error(actual_prices, predicted_prices))\n",
    "\n",
    "# âœ… MAPE ê³„ì‚°\n",
    "mape = mean_absolute_percentage_error(actual_prices, predicted_prices) * 100  # í¼ì„¼íŠ¸ë¡œ ë³€í™˜\n",
    "\n",
    "print(f\"Test RMSE: {rmse:.4f}\")\n",
    "print(f\"Test MAPE: {mape:.2f}%\")\n",
    "\n",
    "# âœ… ë‚ ì§œ ì„¤ì •\n",
    "dates = temp.index[-101:-1]\n",
    "\n",
    "# ðŸ”¹ ê·¸ëž˜í”„ ì¶œë ¥\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(dates, actual_prices, label=\"Actual Prices\", marker=\"o\", linestyle=\"dashed\", color=\"blue\")\n",
    "plt.plot(dates, predicted_prices, label=\"Predicted Prices\", marker=\"s\", linestyle=\"solid\", color=\"red\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Stock Price\")\n",
    "plt.title(f\"{new_ticker} Stock Price Prediction vs Actual\")\n",
    "plt.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
